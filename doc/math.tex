\documentclass[11pt,twocolumn]{amsart} % twocolumn
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{a4paper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}


% shortcuts
\newcommand{\ve}[1]{\boldsymbol{#1}}
\newcommand{\ma}[1]{\boldsymbol{#1}}
\newenvironment{m}{\begin{bmatrix}}{\end{bmatrix}}


\title{Some stuff for Rubyk}
\author{Gaspard Buma}
%\date{}                                           % Activate to display a given date or no date

\begin{document}

\twocolumn[
\maketitle
]
\section{Notations}
In the following discussion the following typographic notations has been used:
\begin{itemize}
  \item \emph{Lower case bold face} means vector (ex. $\ve\mu, \ve{x}$). All vectors are row vectors like $\begin{m} x_1 & x_2 \end{m}$ not $\begin{m} x_1 \\ x_2 \end{m}$.
  \item \emph{Upper case bold face} means matrix (ex. $\ma{A}, \ma{S}, \ma{\Sigma}$).
  \item The transpose of a matrix is noted $\ma{A}^T$, not $\ma{A}'$.
  \item $\ve{x}', \ma{A}'$ denotes a vector/matrix that has been transformed.
\end{itemize}
\section{Variance of a value}

The \textbf{variance} (expressed with the symbol $\sigma^2$ or $\sigma_S^2$) measures the dispersion of a set of values $\ma{S}$ from the mean value $\mu$ or expected value $E(\ma{S})$. The mean value is~:

\[
  \mu = E(\ma{S}) = \frac{1}{l} \sum_1^{l} s_l
\]

$l$ being the number of elements in the set. The variance can be computed as~:
\[
  \sigma^2 = E[(\ma{S} - \mu)(\ma{S} - \mu)] = \frac{1}{l} \sum_1^{l} (s_l - \mu)^2
\]

For example, given the set of values $\ma{S} = [1,4,3,6]$, the mean value $\mu$ is~:
\[
  \mu = \frac{1+4+3+6}{4} = 3.5,
\]

and the variance $\sigma^2$~:
\begin{align*}
  \sigma^2 = & (1-3.5)^2 + (4-3.5)^2 +\\
             & \frac{(3-3.5)^2 + (6-3.5)^2}{4} = \frac{13}{4} = 3.25
\end{align*}

\section{variance-covariance matrix}

The \textbf{variance-covariance matrix} is the generalisation to greater dimensions of the variance seen before. Imagine we have a set $\ma{S}$ of vectors of dimension $n$ and we want to compute the variance of these vectors. We must first compute the expected value $E(\ma{S})$ which is the mean vector $\mu$~:
\begin{align*}
  E(\ma{S}) = & \ve{\mu} = \begin{m} \mu_1, \hdots, \mu_n \end{m} \\
       = & \frac{1}{l} \begin{m} \begin{m} s_{1,1}, \hdots, s_{1,n} \end{m} \\ + \\ \vdots \\ + \\ \begin{m} s_{l,1}, \hdots, s_{l,n} \end{m} \end{m}
\end{align*}

We can build a matrix $\Theta$ (theta) of size ($l,n$) from the vector $\mu$ (mu)~:
\[
  \ma\Theta = \begin{m} \ve{\mu} \\ \vdots \\ \ve{\mu} \end{m}
\]

From the definition of the variance, we have
\[
  \Sigma = E \begin{m} (\ma{S} - \ma\Theta)^T (\textbf{S} - \ma\Theta)\end{m}
\]

thus each entry in the variance-covariance matrix $\ma\Sigma$ (sigma) is~:
\[
  \Sigma_{ij} = \frac{\sum_{k=1}^{l} (s_{ki} - \mu_i)(s_{kj} - \mu_j)}{l}.
\]


For example, given the set $\ma{S}$ formed with three experiments measuring two values~:
\begin{displaymath}
  \ma{S} = \begin{m} 1 & 1 \\ 0 & 4 \\ 2 & 1 \end{m},
\end{displaymath}

we can compute the expected value~:
\[
  \ve{\mu} = \frac{1}{3} \begin{m} \left( \begin{array}{c} 1 \\ + \\ 0 \\ + \\ 2 \end{array} \right) \left( \begin{array}{c} 1 \\ + \\ 4 \\ + \\ 1 \end{array} \right) \end{m} = \begin{m} 1 & 2 \end{m}.
\]


To find variance-covariance matrix, let us first compute the \emph{mean deviation form} of $\ma{S}$~:
\[
  \ma{T} = \ma{S} - \ma\Theta = \begin{m} (1-1) & (1-2) \\ (0-1) & (4-2) \\ (2-1) & (1-2)\end{m} = \begin{m} 0 & -1 \\ -1 & 2 \\ 1 & -1\end{m}.
\]

We can now find $\ma\Sigma$~:
\begin{align*}
  \ma\Sigma = & E \begin{m} \ma{T}^T\ma{T}\end{m} = \frac{1}{n-1} \begin{m} \ma{T}^T\ma{T}\end{m} \\
         = & E \begin{m} \begin{m} 0 & -1 & 1 \\ -1 & 2 & -1\end{m} \begin{m} 0 & -1 \\ -1 & 2 \\ 1 & -1\end{m} \end{m} \\
         = & E \begin{m} 0*0 + -1*-1 + 1*1 & \hdots \\ -1*0 + 2*-1 + -1*1 & \hdots \end{m} \\
         = & E \begin{m} 2 & -3 \\ -3 & 6 \end{m} = \frac{1}{2} \begin{m} 2 & -3 \\ -3 & 6 \end{m}
         = \begin{m} 1 & -\frac{3}{2} \\ -\frac{3}{2} & 3\end{m}
\end{align*}

From the result above, $1$ is the variance along $x$, $3$ is the variance along $y$ and $-\frac{3}{2}$ is the covariance between the two dimensions. There are proofs on why we should use $\frac{1}{n-1}$ for the correct normalization factor instead of the straight-forward $\frac{1}{n}$ but this is beyond the scope of my competences...

\section{Mahalanobis distance}

Let us compute the \textbf{Mahalanobis distance} from a vector $\ve{t}$ to the set $\ma{S}$ with the variance-covariance matrix $\ma\Sigma$ and the mean value $\ve{\mu}$~:
\[
  D_M(\ve{t}) = \sqrt{(\ve{t} - \ve{\mu})\Sigma^{-1}(\ve{t} - \ve{\mu})^T}
\]

If we remove the covariances from the matrix $\ma\Sigma$, we have a diagonal matrix whose entries are the variances (standard deviations) for each dimension ($1$ and $3$ in the example above). The Mahalanobis distance becomes the \textbf{normalized Euclidean distance}~:
\[
  D_{nE}(\ve{t}) = \sqrt{\sum_{i=1}^{n}\frac{(t_i - \mu_i)^2}{\sigma_i^2}}.
\]

From the example we used above we compute the different distances for two vectors $\ve{t_1}$ and $\ve{t_2}$~:

\includegraphics{points.pdf}
\begin{align*}
  \ve{t_1}  = & \begin{m} 2 & 2 \end{m} \\
  \ve{t_2}  = & \begin{m} 1 & 3 \end{m} \\
  \ve{\mu} = & \begin{m} 1 & 2 \end{m} \\
  \Sigma    = & \begin{m} 1 & -\frac{3}{2} \\ -\frac{3}{2} & 3\end{m}
\end{align*}

The \textbf{Euclidean distance} $d$ is the same for both points, even though it is visible that $\ve{t_1}$ is more ``out of the way" then $\ve{t_2}$ ~:
\begin{align*}
  d_1 = & \sqrt{\sum_{i=1}^{n}(t_i - \mu_i)^2} \\
     = & \sqrt{(2-1)^2 + (2-2)^2} = 1 \\
  d_2 = & \sqrt{(1-1)^2 + (3-2)^2} = 1
\end{align*}

The \textbf{normalized Euclidean distance} $d_{nE}$ should show that $\ve{t_2}$ matches the set $\ma{S}$ better then $\ve{t_1}$~:
\begin{align*}
  d_{1nE} = & \sqrt{\frac{(2-1)^2}{1} + \frac{(2-2)^2}{3}} = 1 \\
  d_{2nE} = & \sqrt{\frac{(1-1)^2}{1} + \frac{(3-2)^2}{3}} = \frac{1}{\sqrt{3}} \approx 0.6.
\end{align*}

This is better. We can see that $\ve{t_1}$ is penalized for being out of the way in the wrong direction. The ``penalty" for being badly adjusted along the $x$ axis is $1$ when it is only $\frac{1}{3}$ along the $y$ axis.

Let's now see what the Mahalanobis distance tells us. First we need to compute $\Sigma^{-1}$~:
\[
  \Sigma^{-1} = \frac{1}{(1*3 - -\frac{3}{2}*-\frac{3}{2})} \begin{m} 3 & \frac{3}{2} \\ \frac{3}{2} & 1\end{m} = \begin{m} 4 & 2 \\ 2 & \frac{4}{3} \end{m}
\]

We can now find the distances~:
\begin{align*}
  d_{1M} = & \sqrt{(\ve{t_1} - \ve{\mu})\Sigma^{-1}(\ve{t_1} - \ve{\mu})^T} \\
         = & \sqrt{\begin{m} 1 & 0 \end{m} \begin{m} 4 & 2 \\ 2 & \frac{4}{3}\end{m} \begin{m} 1 \\ 0 \end{m}} \\
         = & \sqrt{\begin{m} 4 & 2 \end{m} \begin{m} 1 \\ 0 \end{m}} = 2\\
  d_{2M} = & \sqrt{\begin{m} 0 & 1 \end{m} \begin{m} 4 & 2 \\ 2 & \frac{4}{3}\end{m} \begin{m} 0 \\ 1 \end{m}} \\
         = & \sqrt{\begin{m} 2 & \frac{4}{3}\end{m} \begin{m} 0 \\ 1 \end{m}} = \frac{2}{\sqrt{3}} \approx 1.2\\
\end{align*}

These distances classify the two vectors in the exactly the same way as the normalized Euclidean distances. Why is that so ? The reason is that our vectors are always away from the mean value in a single direction at a time, thus the ``covariance'' part of the error is never used.

What happens with the two points $\ve{t_3}$ and $\ve{t_4}$ ? From the first look at the picture we can see that their distance to the mean value is the same~:
\begin{align*}
  d_3 = & \frac{1}{\sqrt{2}} \approx 0.7 \\
  d_4 = & \frac{1}{\sqrt{2}} \approx 0.7
\end{align*}

Their normalized Euclidean values are also the same as they differ from the mean value exactly in the same way along $x$ and $y$~:
\begin{align*}
  d_{3nE} = & \frac{1}{\sqrt{3}} \approx 0.6\\
  d_{4nE} = & \frac{1}{\sqrt{3}} \approx 0.6
\end{align*}

Now comes the Mahalanobis distances~:
\begin{align*}
  d_{3M} = & \frac{1}{\sqrt{3}} \approx 0.6\\
  d_{4M} = & \frac{\sqrt{14}}{\sqrt{6}} \approx 1.5
\end{align*}

This type of distance calculation is the only one that reflect the difference between the two vectors $\ve{t_3}$ and $\ve{t_4}$ and classifies $\ve{t_4}$ as beeing further ``away'' then $\ve{t_3}$, like our eyes tell us from a quick look at the picture~!

\section{Eigenvalues}

Before we study \emph{Principal Component Analysis}, we need some understanding of eigenvalues. Imagine we have a transformation matrix $\ma{P}$~:
\[
  \ma{P} = \begin{m} 1 & 1 \\ 0 & 2 \end{m}.
\]

\includegraphics{eigen.pdf}

What are the vectors in this transformation that keep their direction (are not rotated) ? Mathematically speaking we want to find $\ve{e}$ such that~:
\[
  \ve{e'} = \ve{e} * \ma{P} = \lambda \ve{e}
\]

We can see from the picture that $\ve{y}$ satisfy this as $\ve{y'}$ is just $2 * \ve{y}$. Such vectors are called \emph{eigenvectors} and their corresponding lambda values \emph{eigenvalues}. To find these vectors, we can rewrite the equation above~:

\[
  \ve{e'} = \ve{e} * \ma{P} = \lambda \ve{e}
\]

as

\[
  \ve{e} * \ma{P} - \lambda \ve{e} = 0
\]

or 

\[
  \ve{e} (\ma{P} - \lambda \ma{I}) = 0  
\]

We know that an equation of the form~:
\[
  \ve{x} \ma{A} = 0
\]
has only the trivial solution $\ve{x} = \ve{0}$ if the determinant of $\ma{A}$ is not null. We can therefore find the non-trivial solutions to the equation above by computing the determinant when the latter \emph{is} null. This equation is called the \emph{characteristic equation} of $\ma{P}$~:
\[
  det(\ma{P} - \lambda \ma{I}) = 0
\]

We can now calculate the values of $\lambda$~:
\begin{align*}
  det(\begin{m} 1 & 1 \\ 0 & 2\end{m} - \lambda \begin{m} 1 & 0 \\ 0 & 1 \end{m}) = & 0 \\
  det(\begin{m} 1-\lambda & 1 \\ 0 & 2-\lambda\end{m}) = & 0 \\
  (1-\lambda)(2-\lambda) - 1 * 0 = & 0 \\
  (1-\lambda)(2-\lambda) = & 0
\end{align*}

which has the solutions
\[
\lambda_1 = 1, \lambda_2 = 2
\]

The corresponding \emph{eigenvectors} can be found by substituting $\lambda_1$ and $\lambda_2$ in $\ve{e} (\ma{P} - \lambda \ma{I}) = \ve{0}$~:
\begin{align*}
  \begin{m} x & y \end{m} \begin{m} 1-\lambda & 1 \\ 0 & 2-\lambda \end{m} = & 0 \\
  \begin{m} x_1 & y_1 \end{m} \begin{m} 1-1 & 1 \\ 0 & 2-1 \end{m} = & 0 \\
  \begin{m} x_1 & y_1 \end{m} \begin{m} 0 & 1 \\ 0 & 1 \end{m} = & 0 \\
  x_1 + y_1 = & 0 \\
  \ve{e_1} = & \begin{m} t & -t \end{m}
\end{align*}

and for $\lambda_2$~:
\begin{align*}
  \begin{m} x_2 & y_2 \end{m} \begin{m} 1-2 & 1 \\ 0 & 2-2 \end{m} = & 0 \\
  \begin{m} x_2 & y_2 \end{m} \begin{m} -1 & 1 \\ 0 & 0 \end{m} = & 0 \\
  -x_2  = & 0 \\
   x_2  = & 0 \\
  \ve{e_2} = \begin{m} 0 & t \end{m} = \ve{y}
\end{align*}

If we look at our transformation picture again, we see that the relation in the direction $\ve{e_1}$ is kept and the relation in the direction of $\ve{e_2}$ is just doubled. This reflects the eigenvalues $1$ and $2$ with the directions of the eigenvectors $\ve{e_1}$ and $\ve{e_2}$.


Before moving on to the following section, we need to establish some properties of \emph{symmetric} matrices. The first one is that such a matrix $\ma{A}$ can be expressed by the multiplication of a diagonal matrix $\ma{D}$ (all entries not on the diagonal are $\ve{0}$) and an invertible matrix $\ma{E}$~:
\begin{equation}
  \ma{A} = \ma{E}^T\ma{D}\ma{E}. \label{symmetric}
\end{equation}

Because $\ma{A}$ is symmetric, the transpose $\ma{A}^T$ of $\ma{A}$ (mirror across the diagonal) is the same as $\ma{A}$, we can therefore write~:
\begin{align*}
  \ma{A}^T = & (\ma{E}^T\ma{D}\ma{E})^T \\
           = &  \ma{E}^T(\ma{E}^T\ma{D})^T \\
           = &  \ma{E}^T\ma{D}^T\ma{E}^{TT}
\end{align*}
now we simply replace $\ma{D}^T$ by $\ma{D}$ since a diagonal matrix is equivalent to its transpose and we replace $\ma{E}^{TT}$ by $\ma{E}$ since transposing twice is equivalent to doing nothing. We get
\[
  \ma{A}^T = \ma{E}^T\ma{D}\ma{E} = \ma{A}.
\]

This proves that any matrix that can be expressed as a product of an invertible matrix $\ma{E}$ and a diagonal matrix $\ma{D}$ is indeed symmetric. We should now prove the reverse~: that any symmetric matrix can be expressed as stated above. This proof is called the \emph{spectral theorem}. While trying to understand it, I fell across strange names like \emph{Hermitian matrices}, \emph{Schur decomposition} and \emph{Hilbert spaces}. It also involved matrix calculations with complex numbers. That was too much me. Sorry. For those interested, I found a proof which you can consult at \emph{(http://users.utu.fi / jkari / compression / kltaddition.pdf)}.

Anyway, let's pass on to the interesting part~: the matrix $\ma{E}$ used in the little talk above is in fact a matrix of the orthonormal eigenvectors of $\ma{A}$ (those vectors that do not rotate when transformed by $\ma{A}$). The proof is interesting but quite long, you can safely pass on to the section on \emph{Principal Component Analysis} if you do not have fun with letters in capital bold.

\emph{Please note that the following proofs are a rewrite of the first appendix of the tutorial on PCA by Jonathon Shlens\footnote{www.snl.salk.edu/~shlens/pub/notes/pca.pdf}}.

First, lets build this matrix $\ma{E}$ from the eigenvectors of $\ma{A}$~:
\[
  \ma{E} = \begin{m} \ve{e_1} \\ \vdots \\ \ve{e_n} \end{m}.
\]

Now let $\ma{D}$ be a diagonal matrix with the corresponding eigenvalues of $\ma{A}$~:
\[
  \ma{D} = \begin{m} \lambda_1 & 0 & .. \\ 0 & \lambda_2 & .. \\ \vdots & \vdots & \ddots \end{m}.
\]

We first try to prove that $\ma{E}\ma{A} = \ma{D}\ma{E}$~:
\begin{align*}
  \ma{E}\ma{A} = \begin{m} \ve{e_1} \\ \ve{e_2} \\ \vdots \end{m} \ma{A} = & 
                 \begin{m} \ve{e_1}\ma{A} \\ \ve{e_2}\ma{A} \\ \vdots \end{m} \\
  \ma{D}\ma{E} = \begin{m} \lambda_1 & 0 & .. \\ 0 & \lambda_2 & .. \\ \vdots & \vdots & \ddots \end{m} \ma{E} = & 
                 \begin{m} \lambda_1\ve{e_1} \\ \lambda_2\ve{e_2} \\ \vdots \end{m}.
\end{align*}

For $\ma{E}\ma{A}$ to be equal to $\ma{D}\ma{E}$, we must show that $\ve{e_i}\ma{A} = \lambda_i\ve{e_i}$ for all $i$. Remember that an eigenvector is \emph{a vector that is scaled onto itself by the transformation}, that is $\ve{e'} = \lambda\ve{e}$. This is exactly what the result of our equation states, thus the entries of $\ma{E}$ are the eigenvectors of $\ma{A}$ if $\ma{E}\ma{A} = \ma{D}\ma{E}$.

To finish the proof, we just need to show that $\ma{E}\ma{A} = \ma{D}\ma{E}$ implies $\ma{A} = \ma{E}^T\ma{D}\ma{E}$. By multiplying each side by $\ma{E}^{-1}$, we obtain $\ma{A} = \ma{E}^{-1}\ma{D}\ma{E}$. If we could show that $\ma{E}^{-1} = \ma{E}^T$, we would have proven (\ref{symmetric}). We know that $\ma{E}^{-1} = \ma{E}^T$ if $\ma{E}$ is an orthogonal matrix since~:
\begin{align*}
  \ma{E}\ma{E}^T = & \begin{m} \ve{e_1} \\ \ve{e_2} \\ \vdots \end{m} \begin{m} \ve{e_1} & \ve{e_2} & \hdots \end{m} \\
                 = & \begin{m} \ve{e_1}\ve{e_1} & \ve{e_1}\ve{e_2} & \hdots \\ \ve{e_2}\ve{e_1} & \ve{e_2}\ve{e_2} & \hdots \\ \vdots & \vdots & \ddots \end{m} \\
                 = & \begin{m} 1 & 0 & \hdots \\ 0 & 1 & \hdots \\ \vdots & \vdots & \ddots \end{m} \\
                 = & \ma{I}
\end{align*}

because each vectors in an orthogonal matrix is $\perp$ to other vectors, thus the dot product is zero except for the diagonal entries where it is equal to the squared norm of the vector. If we choose our eigenvectors so that their norm is 1, we obtain the identity matrix $\ma{I}$. We just need to show that $\ma{E}$ is an orthogonal matrix and pulling the thread out of this maze, we prove that a symmetric matrix can be expressed by (\ref{symmetric}). We want to know if $\vec(e_i) \dot \vec{e_j} = 0$ for all $i \not= j$. Let $\lambda_i$ and $\lambda_j$ be two distinct eigenvalues.
\begin{align*}
  \lambda_i\ve{e_i} \cdot \ve{e_j} = & \lambda_i\ve{e_i}\ve{e_j}^T \\
                                  = & \ve{e_i}\ma{A}\ve{e_j}^T \\
                                  = & \ve{e_i}(\ve{e_j}\ma{A}^T)^T \\
                                  = & \ve{e_i}(\ve{e_j}\ma{A})^T \\
                                  = & \ve{e_i}(\lambda_j\ve{e_j}^T) \\
  \lambda_i\ve{e_i} \cdot \ve{e_j} = & \ve{e_i} \cdot \lambda_j\ve{e_j}
\end{align*}

Thus $(\lambda_i - \lambda_j)\ve{e_i} \cdot \ve{e_j} = 0$. Since we have conjectured that $\lambda_i \not= \lambda_j$, it must be that $\ve{e_i}$ and $\ve{e_j}$ are orthonormal (their dot product is $0$). Therefore $\ma{E}^{-1} = \ma{E}^T$ and we can write~:
\[
  \ma{A} = \ma{E}^T\ma{D}\ma{E}.
\]

All this work, just to reexpress an arbitrary symmetric matrix $\ma{A}$\dots But this result is the ground on which the next section is built.

\section{Principal Component Analysis}

Principal Component Analysis is a way to extract the ``most proeminent features'' of a data set. It assumes that the data has a good signal to noise ratio which means that \emph{great variance} in the data is a synonym for \emph{relevant feature} and not \emph{big noise}.

In all the calculations below, we need to use the \emph{variations} of the measures. We will thus remove the mean values from our data set and use this new set called the \emph{mean deviation form} of $\ma{S}$~:
\[
  \ma{T} = \ma{S} - \begin{m}\ve{\mu} \\ \vdots \\ \ve{\mu}\end{m}.
\]

In order to extract the ``most proeminent features'' of our data, a good idea would be to look at the directions in which the data varies most. Is it along the $\ve{x}$ axis ? The $\ve{y}$ axis ? Somewhere in between ?

Recall from the previous sections that the variance-covariance matrix $\ma\Sigma$ displays variances along the diagonal and covariances off-diagonal. We can use this matrix to extract directions in which the data varies most~:
\[
  \ma\Sigma = \frac{1}{n-1}\ma{T}^T\ma{T}.
\]

By looking at the diagonals of $\Sigma$, we pick the greatest absolute value and there we are, we found our direction ($3 \Rightarrow \ve{y}$). Is that it ? No, this solution misses the fact that the main direction might not follow one of the axis (dimension) of our recorded data (the ellipsoid is not vertical nor horizontal). What we need is to apply a transformation (rotation + stretch) to our data $\ma{T}$ into an new basis $\ma{P}$ prior to looking at the variance-covariance matrix, so that the principal components align with the new basis' axis.
\begin{align*}
  \ma{T'} = & \ma{T}\ma{P} \\
  \ma\Sigma'     = & \frac{1}{n-1}\ma{T'}^T\ma{T'}
\end{align*}

Our goal is thus to find a new basis $\ma{P}$ that gives us great absolute values along the diagonals of $\ma\Sigma'$ and small absolute values off-diagonal (small redundancy). If possible, it would be great to see large variances along the top of the diagonal which would mean we could just cut off the rightmost part of the matrix $\ma{T'}$ and still have most of the information on the varying signal. To summarize~:
\begin{itemize}
  \item Find a new basis $\ma{P}$ that reduces the redundancy of the signals (low covariance).
  \item The $\ma\Sigma'$ matrix for $\ma{T'}$ should ideally be a diagonal matrix with very high values at the top and very small at the bottom, thus showing signals in the first dimensions and pure noise in the last ones.
  \item We could then choose to remove the columns of $\ma{T'}$ that have greater indices than the number of dimensions we want to keep thus reducing the dimensionality of our data.
\end{itemize}

A simple way to find the solution would be~:
\begin{enumerate}
  \item Find the direction $\ve{e_1}$ that shows the greatest variance in the data $\ma{T}$. Normalize this vector. This is our first basis vector.
  \item Find another normalized vector showing the greatest variance that is orthogonal to those already found ($\forall \ve{b}\in \{\text{found vectors}\}, \ve{e_n} \perp \ve{b}$).
  \item Repeat until we have $n$ basis vectors ($n$ is the size of vector space).
\end{enumerate}

The resulting ordered set of vectors are the \emph{principal components} of our data set \textbf{T}.

\section{computing PCA}

Mathematically stated, what we are looking for is a diagonalized version of $\ma\Sigma'$~:
\begin{align*}
  \ma\Sigma'     = & \frac{1}{n-1}\ma{T'}^T\ma{T'} \\
                 = & \frac{1}{n-1}(\ma{T}\ma{P})^T(\ma{T}\ma{P}) \\
                 = & \frac{1}{n-1}\ma{P}^T\ma{T}^T\ma{T}\ma{P}
\end{align*}

Does this look familiar to you ? Recall all our lengthy discussion on symmetrical matrices ? Well, $\ma{T}^T\ma{T}$ is such a matrix. Lets replace it by $\ma{A}$. We get~:
\[
  \ma\Sigma'     = \frac{1}{n-1} \ma{P}^T\ma{A}\ma{P}
\]

Now what happens if we set $\ma{P}^T = \ma{E}$ and replace $\ma{A}$ by $\ma{E}^T\ma{D}\ma{E}$ (\ref{symmetric}) ?
\begin(align*)
  (n-1)\ma\Sigma' = & \frac{1}{n-1}\ma{E}\ma{E}^T\ma{D}\ma{E}\ma{E}^T \\
                  = & \frac{1}{n-1}\ma{I}\ma{D}\ma{I}
                  = & \frac{\ma{D}}{n-1}
\end{align*}

We have just found our diagonalized variance-covariance matrix by using the transpose of $\ma{E}$ which is the matrix of eigenvectors of $\ma{T}^T\ma{T}$ !

\section{applying PCA to \textbf{S}}

Using the same sample data as above, we imagine that the set of points $\textbf{S}$ represents the measures of a moving element. The object actually moves only along the ellipsoid axis $\ve{e} = (-1,2)$, the deviations from this axis is just noise. The goal of PCA is to re-express our set of data in a new basis that reflects this behaviour (data along $\ve{e}$, noise along the other direction). The naive basis (in which we recorded the data) is~:
\[
  \textbf{B} = \begin{m} \ve{x} \\ \ve{y} \end{m} = \begin{m} 1 & 0 \\ 0 & 1 \end{m} = \textbf{I}
\]

We will use PCA to find a new basis $\textbf{P}$ that is a linear combination of the original matrix $\textbf{B}$ and in which we will only look at the data along the greatest axis of the ellipsoid (where the data exhibits the greatest variance). In this new basis, the set of data \textbf{B} becomes~:
\[
  \textbf{S'} = \textbf{S}\textbf{P}
\]

The new basis should optimise signal to noise ratio if we consider the first dimension as representing the signal and the second the noise~:
\[
  SNR = \frac{\sigma_{signal}^2}{\sigma_{noise}^2}
\]

Before delving any further, we must first transform our data into its \emph{mean deviation form} $\textbf{T}$. 
\[
  \textbf{T} = \textbf{S} - \begin{m}\ve{\mu} \\ \vdots \\ \ve{\mu}\end{m} = \begin{m} 0 & -1 \\ -1 & 2 \\ 1 & -1\end{m}.
\]

We can now easily compute the actual SNR~:
\[
  SNR = \frac{\sum_{k=1}^l T_{k1}^2}{\sum_{k=1}^l T_{k2}^2} = \frac{0+1+1}{1+4+1} = \frac{1}{3}
\]

How do we find the new basis that gives us the greatest SNR ? Expressed in another way, we could say~: how do we find the basis that shows greatest variance in the first dimension and mostly noise in the second ?

\section{notes on PCA}

For a deaper understanding of PCA and it's limits, one may have a look at~:
\begin{itemize}
  \item \emph{kernel PCA} to apply a non-linear filtre prior to applying PCA for problems where linearity is an issue.
  \item \emph{ICA} for sets of data whose distribution probabilities are not exponential (Gaussian, Exponential, etc).
  \item \emph{Central Limit Theorem} on why PCA should work most of the time in real world as probabilities usually \textbf{are} gaussian.
\end{itemize}

\onecolumn

\section{Ruby code}
\subsection{variance}
\begin{verbatim}
set   = [1,4,3,6]
mu    = set.inject(0) {|s,v| s + v } / set.size.to_f
sigma = set.inject(0) {|s,v| s + ((v - mu)**2) } / set.size.to_f
\end{verbatim}

\subsection{Mahalanobis distance}
\begin{verbatim}
class Array
  # transpose
  def t
    cols = []
    self[0].each_index do |i|
      cols[i] = map {|v| v[i]}
    end
    cols
  end

  def *(m)
    res = []
    each_index do |row|
      res[row] = []
      m[0].each_index do |col|
        res[row][col] = 0
        self[0].each_index do |i|
          res[row][col] += self[row][i] * m[i][col]
        end
      end
    end
    res
  end

  def det
    (self[0][0] * self[1][1]) - (self[1][0] * self[0][1])
  end

  def /(scalar)
    map {|row| row.map {|v| v / scalar}}
  end

  # vector substraction
  def -(v)
    res = [[]]
    self[0].each_index {|i| res[0][i] = self[0][i] - v[0][i]}
    res
  end

  def to_s
    if self.size > 1 || self[0].size > 1
      if self.size == 1
        row_format = '[' + (" %3.1f" * self[0].size) + " ]\n"
      else
        row_format = '|' + (" %3.1f" * self[0].size) + " |\n"
      end
      self.inject("") {|s,row| s + sprintf(row_format, *row)}
    else
      self[0][0].to_s
    end
  end

  def map_index(&block)
    res = []
    self.each_index do |i|
      res[i] = yield(i,self[i])
    end
    res
  end
end


set = [[1,1],
       [0,4],
       [2,1]]
# mean vector mu
mu = [[]]
set[0].each_index do |i|
  mu[0][i] = set.inject(0) {|s,v| s + v[i] } / set.size.to_f
end

# set - mu
s_mu = set.map do |v|
  r = []
  v.each_index do |i|
    r[i] = v[i] - mu[0][i]
  end
  r
end

# variance-covariance matrix
sigma = (s_mu.t * s_mu) / (set.size - 1.0).to_f

# inverse of sigma
if sigma.det == 0
  puts "Matrix not invertible"
  return
end

# invers of sigma (only for a 2x2 matrix)
sigma_inv = [[  sigma[1][1], -sigma[0][1] ],
             [ -sigma[1][0],  sigma[0][0] ]] / sigma.det

puts "Sigma\n\n"
puts sigma.to_s
puts "\nSigma's inverse\n\n"
puts sigma_inv.to_s

puts "\n\nAll results express value^2 (before square root)"

[
[[[2  , 2  ]],
[ [1  , 3  ]]],
[[[1.5, 1.5]],
[ [1.5, 2.5]]]
].each do |t1,t2|
  puts "\n-----------------------------"
  puts "t1   = #{t1}"
  puts "t2   = #{t2}"
  puts "t1-u = #{t1 - mu}"
  puts "t2-u = #{t2 - mu}\n\n"

  d1 = (t1 - mu).flatten.inject(0) {|s,v| s + v**2} 
  d2 = (t2 - mu).flatten.inject(0) {|s,v| s + v**2}

  puts "d1   = #{d1}"
  puts "d2   = #{d2}\n\n"

  d1nE = (t1 - mu).flatten.map_index{|i,v| v**2 / sigma[i][i] }.inject(0) {|s,v| s + v } 
  d2nE = (t2 - mu).flatten.map_index{|i,v| v**2 / sigma[i][i] }.inject(0) {|s,v| s + v } 

  puts "d1nE = #{d1nE}"
  puts "d2nE = #{d2nE}\n\n"

  d1M = (t1 - mu) * sigma_inv * (t1 - mu).t
  d2M = (t2 - mu) * sigma_inv * (t2 - mu).t

  puts "d1M  = #{d1M}"
  puts "d2M  = #{d2M}\n\n"
end
\end{verbatim}
\end{document}